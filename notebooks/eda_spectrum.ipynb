{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Final\n",
    "\n",
    "import mlflow\n",
    "import mlflow.entities\n",
    "import mlflow.data.pandas_dataset\n",
    "from mlflow.data.sources import (\n",
    "    LocalArtifactDatasetSource\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "import pickle\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_MIN    = np.iinfo(np.int8).min\n",
    "INT8_MAX    = np.iinfo(np.int8).max\n",
    "INT16_MIN   = np.iinfo(np.int16).min\n",
    "INT16_MAX   = np.iinfo(np.int16).max\n",
    "INT32_MIN   = np.iinfo(np.int32).min\n",
    "INT32_MAX   = np.iinfo(np.int32).max\n",
    "\n",
    "FLOAT16_MIN = np.finfo(np.float16).min\n",
    "FLOAT16_MAX = np.finfo(np.float16).max\n",
    "FLOAT32_MIN = np.finfo(np.float32).min\n",
    "FLOAT32_MAX = np.finfo(np.float32).max\n",
    "\n",
    "def memory_usage(data, detail=1):\n",
    "    if detail:\n",
    "        display(data.memory_usage())\n",
    "    memory = data.memory_usage().sum() / (1024*1024)\n",
    "    print(\"Memory usage : {0:.2f}MB\".format(memory))\n",
    "    return memory\n",
    "\n",
    "def compress_dataset(data):\n",
    "    \"\"\"\n",
    "        Compress datatype as small as it can\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: pandas Dataframe\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "    \"\"\"\n",
    "    memory_before_compress = memory_usage(data, 0)\n",
    "    print()\n",
    "    length_interval      = 50\n",
    "    length_float_decimal = 4\n",
    "\n",
    "    print('='*length_interval)\n",
    "    for col in data.columns:\n",
    "        col_dtype = data[col][:100].dtype\n",
    "\n",
    "        if col_dtype != 'object':\n",
    "            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n",
    "            col_series = data[col]\n",
    "            col_min = col_series.min()\n",
    "            col_max = col_series.max()\n",
    "\n",
    "            if col_dtype == 'float64':\n",
    "                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, length_float_decimal)), str(np.round(col_max, length_float_decimal))))\n",
    "                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n",
    "                    print(\"compress float64 --> float16\")\n",
    "                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n",
    "                    print(\"compress float64 --> float32\")\n",
    "                else:\n",
    "                    pass\n",
    "                memory_after_compress = memory_usage(data, 0)\n",
    "                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n",
    "                print('='*length_interval)\n",
    "\n",
    "            if col_dtype == 'int64':\n",
    "                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n",
    "                type_flag = 64\n",
    "                if (col_min > INT8_MIN/2) and (col_max < INT8_MAX/2):\n",
    "                    type_flag = 8\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n",
    "                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n",
    "                    type_flag = 16\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n",
    "                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n",
    "                    type_flag = 32\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n",
    "                    type_flag = 1\n",
    "                else:\n",
    "                    pass\n",
    "                memory_after_compress = memory_usage(data, 0)\n",
    "                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n",
    "                if type_flag == 32:\n",
    "                    print(\"compress (int64) ==> (int32)\")\n",
    "                elif type_flag == 16:\n",
    "                    print(\"compress (int64) ==> (int16)\")\n",
    "                else:\n",
    "                    print(\"compress (int64) ==> (int8)\")\n",
    "                print('='*length_interval)\n",
    "\n",
    "    print()\n",
    "    memory_after_compress = memory_usage(data, 0)\n",
    "    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = os.getenv(\"TRACKING_USER\")\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = os.getenv(\"TRACKING_PSWD\")\n",
    "os.environ[\"MLFLOW_HTTP_REQUEST_TIMEOUT\"] = \"9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f\"http://{os.getenv('ADRESS')}:{os.getenv('PORT')}\")\n",
    "CURRENT_EXPERIMENT: mlflow.entities.Experiment = mlflow.set_experiment(\"Spectrum_EDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SAMPLE, SPECTRUM\n",
    "# ----------\n",
    "SAMPLE_PATH: str = \"../data/raw/spectr/Выборка_Н22_5_1.csv\"\n",
    "SAMPLE_REST: str = \"../data/raw/spectr/СКЗУ_ВЫБОРКА_ЯНВАР-МАРТ.csv\"\n",
    "# HISTORICAL DATA\n",
    "# ----------\n",
    "RMSA_PATH: str = \"../data/raw/historical/СКЗУ.csv\"\n",
    "RMSA10_PATH: str = \"../data/raw/historical/СКЗУ10.csv\"\n",
    "RMSA_GROWTH: str = \"../data/raw/historical/СКЗУ_РОСТ.csv\"\n",
    "RMSA_AMP: str = \"../data/raw/historical/СКЗУ_АМПЛИТУДА.csv\"\n",
    "RMSD_SPAN: str = \"../data/raw/historical/СКЗП_РАЗМАХ.csv\"\n",
    "PK_FACTOR: str = \"../data/raw/historical/ПИК_ФАКТОР.csv\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATH, skiprows=16, sep=';')\n",
    "df.drop(columns=df.columns[[0, 1]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset w/o dates\n",
    "column_names = pd.to_datetime(df.iloc[0, ::2].tolist(), format='%d.%m.%Y %H:%M:%S') \n",
    "df = df.iloc[:, 1::2]\n",
    "df.columns = column_names\n",
    "df = df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to float\n",
    "df = df.apply(lambda x:\n",
    "              pd.to_numeric(\n",
    "                  x.str.replace(',','.'),\n",
    "                  errors='coerce')\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF CHANGES MADE CHANGE FILENAME: SAMPLE_1, SAMPLE_2, ...\n",
    "SAMPLE_LOG_PATH: Final[str] = \"../data/cleaned/spectr/sample.csv\"\n",
    "# TO LOG ONLY ONCE\n",
    "if not os.path.isfile(SAMPLE_LOG_PATH):\n",
    "    df.to_csv(\"../data/cleaned/spectr/sample.csv\", index=False)\n",
    "else:\n",
    "    # IMPLEMENT A COMPARISON OF 2 VERSIONS OF DATASET\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().to_html(\"./artifacts/stats.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF columns to string to log dataset\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "RUN_NAME: str = \"Sample dataset\"\n",
    "\n",
    "# check if experiment contains run with such name\n",
    "# if so, then change run name (manually) and log dataset\n",
    "# else log dataset\n",
    "\n",
    "# GET ALL RUNS OF EXPERIMENT\n",
    "# ----------\n",
    "runs = mlflow.search_runs([CURRENT_EXPERIMENT.experiment_id])\n",
    "# CHECK IF EXPERIMENT \n",
    "# ----------\n",
    "if runs[runs[\"tags.mlflow.runName\"] == RUN_NAME].shape[0] == 0:\n",
    "    dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "        df, \n",
    "        name=\"Raw Samples\",\n",
    "        source=\"\"\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        # Log dataset stats\n",
    "        mlflow.log_artifact(local_path=\"./artefacts/stats.html\")\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"EDA\")\n",
    "\n",
    "    RUN_ID = run.info.run_id\n",
    "    logged_run = mlflow.get_run(run.info.run_id)\n",
    "\n",
    "    # Retrieve the Dataset object\n",
    "    logged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n",
    "\n",
    "    # View some of the recorded Dataset information\n",
    "    print(f\"Dataset name: {logged_dataset.name}\")\n",
    "    print(f\"Dataset digest: {logged_dataset.digest}\")\n",
    "    print(f\"Dataset profile: {logged_dataset.profile}\")\n",
    "    print(f\"Dataset schema: {logged_dataset.schema}\")\n",
    "else:\n",
    "    # IMPLEMENT CHANGING OF RUN NAME\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF CHANGES MADE CHANGE FILENAME: SAMPLE_1, SAMPLE_2, ...\n",
    "SAMPLE_COMPRESSED_LOG_PATH: Final[str] = \"../data/processed/spectr/compressed.csv\"\n",
    "# TO LOG ONLY ONCE\n",
    "if not os.path.isfile(SAMPLE_COMPRESSED_LOG_PATH):\n",
    "    df.to_csv(SAMPLE_COMPRESSED_LOG_PATH, index=False)\n",
    "else:\n",
    "    # IMPLEMENT A COMPARISON OF 2 VERSIONS OF DATASET\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF columns to string to log dataset\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "RUN_NAME: str = \"Compressed dataset\"\n",
    "\n",
    "# check if experiment contains run with such name\n",
    "# if so, then change run name (manually) and log dataset\n",
    "# else log dataset\n",
    "\n",
    "# GET ALL RUNS OF EXPERIMENT\n",
    "# ----------\n",
    "runs = mlflow.search_runs([CURRENT_EXPERIMENT.experiment_id])\n",
    "# CHECK IF EXPERIMENT EXISTS\n",
    "# ----------\n",
    "if runs[runs[\"tags.mlflow.runName\"] == RUN_NAME].shape[0] == 0:\n",
    "    dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "        df, \n",
    "        name=\"Compressed types\",\n",
    "        source=LocalArtifactDatasetSource(\"C:/Users/egore/Desktop/predictive_RUL/data/processed/spectr/compressed.csv\")\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"COMPRESSED_DATA_EDA\")\n",
    "\n",
    "\n",
    "    RUN_ID = run.info.run_id\n",
    "    logged_run = mlflow.get_run(run.info.run_id)\n",
    "\n",
    "    # Retrieve the Dataset object\n",
    "    logged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n",
    "\n",
    "    # View some of the recorded Dataset information\n",
    "    print(f\"Dataset name: {logged_dataset.name}\")\n",
    "    print(f\"Dataset digest: {logged_dataset.digest}\")\n",
    "    print(f\"Dataset profile: {logged_dataset.profile}\")\n",
    "    print(f\"Dataset schema: {logged_dataset.schema}\")\n",
    "else:\n",
    "    # IMPLEMENT CHANGING OF RUN NAME\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df.T.index\n",
    "feature_dataset = pd.DataFrame()\n",
    "feature_dataset[\"kurtosis\"] = df.apply(lambda x: stats.kurtosis(x.astype(np.float64)))\n",
    "feature_dataset[\"skewness\"] = df.apply(lambda x: stats.skew(x.astype(np.float64)))\n",
    "feature_dataset[\"peak2peak\"] = df.apply(lambda x: np.ptp(x.astype(np.float64)))\n",
    "feature_dataset[\"mean\"] = df.apply(lambda x: x.astype(np.float64).mean())\n",
    "feature_dataset[\"std\"] = df.apply(lambda x: x.astype(np.float64).std())\n",
    "feature_dataset[\"shapeFactor\"] = df.apply(lambda x: np.sqrt(np.mean(np.square(x.astype(np.float64)))) / np.abs(x.astype(np.float64).mean()))\n",
    "feature_dataset[\"energy\"] = df.apply(lambda x: np.sum(x.astype(np.float64) ** 2))\n",
    "feature_dataset.index = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF CHANGES MADE CHANGE FILENAME: SAMPLE_1, SAMPLE_2, ...\n",
    "SAMPLE_FEATURE_LOG_PATH: Final[str] = \"../data/processed/spectr/feature_dataset.csv\"\n",
    "# TO LOG ONLY ONCE\n",
    "if not os.path.isfile(SAMPLE_FEATURE_LOG_PATH):\n",
    "    feature_dataset.to_csv(SAMPLE_FEATURE_LOG_PATH, index=False)\n",
    "else:\n",
    "    # IMPLEMENT A COMPARISON OF 2 VERSIONS OF DATASET\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run id used, cuz i fucked up run names\n",
    "RUN_ID = \"3fbc94286d0a4bf889137e5dc99704ca\"\n",
    "with mlflow.start_run(run_id=RUN_ID) as run:\n",
    "    fig, ax = plt.subplots(7, figsize=(16,16))\n",
    "    for i, col in enumerate(feature_dataset.columns):\n",
    "        sns.lineplot(feature_dataset[col], ax=ax[i])\n",
    "        ax[i].set_xticklabels(\"\")\n",
    "    plt.savefig(\"./artifacts/feature_dataset.png\")\n",
    "    mlflow.log_figure(fig, \"./artifacts/feature_dataset.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"3fbc94286d0a4bf889137e5dc99704ca\") as run:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_dataset)\n",
    "\n",
    "    with open(\"./artifacts/scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    mlflow.log_artifact(\"./artifacts/scaler.pkl\", \"artifacts/utils/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"3fbc94286d0a4bf889137e5dc99704ca\") as run:\n",
    "    pca = PCA().fit(scaled_features)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    ax.axhline(0.95, c=\"red\")\n",
    "    ax.set_xlabel('number of components')\n",
    "    ax.set_ylabel('cumulative explained variance')\n",
    "    ax.set_title(\"Number of components vs explained variance\")\n",
    "\n",
    "    plt.savefig(\"./artifacts/pca_var.png\")\n",
    "\n",
    "    mlflow.log_figure(fig, \"artifacts/pca_var.png\")\n",
    "\n",
    "    with open(\"./artifacts/pca.pkl\", \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    mlflow.log_artifact(\"./artifacts/pca.pkl\", \"artifacts/utils/pca.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create new run\n",
    "* Log feature dataset  \n",
    "* log scaled dataset  \n",
    "* log pca3 & pca4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[runs[\"tags.mlflow.runName\"] == RUN_NAME]['run_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF columns to string to log dataset\n",
    "feature_dataset.columns = feature_dataset.columns.astype(str)\n",
    "\n",
    "RUN_NAME: str = \"PCA_SEACH_INDICATOR\"\n",
    "\n",
    "# GET ALL RUNS OF EXPERIMENT\n",
    "# ----------\n",
    "runs = mlflow.search_runs([CURRENT_EXPERIMENT.experiment_id])\n",
    "# CHECK IF EXPERIMENT EXISTS\n",
    "# ----------\n",
    "dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "    feature_dataset, \n",
    "    name=\"FEATURE DATASET\",\n",
    "    source=\"\"\n",
    ")\n",
    "if runs[runs[\"tags.mlflow.runName\"] == RUN_NAME].shape[0] == 0:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"SEARCH HEALTH INDICATOR\")\n",
    "        # Log dataset csv\n",
    "        mlflow.log_artifact(\"../data/processed/spectr/feature_dataset.csv\", \"artifacts/data/feature_dataset.csv\")\n",
    "        # Log dataset as JSON\n",
    "        mlflow.log_table(feature_dataset, \"artifacts/data_json/feature_dataset.json\")\n",
    "else:\n",
    "    with mlflow.start_run(run_id=runs[runs[\"tags.mlflow.runName\"] == RUN_NAME]['run_id'][0]):\n",
    "        ...\n",
    "\n",
    "RUN_ID = run.info.run_id\n",
    "logged_run = mlflow.get_run(run.info.run_id)\n",
    "\n",
    "# Retrieve the Dataset object\n",
    "logged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n",
    "\n",
    "# View some of the recorded Dataset information\n",
    "print(f\"Dataset name: {logged_dataset.name}\")\n",
    "print(f\"Dataset digest: {logged_dataset.digest}\")\n",
    "print(f\"Dataset profile: {logged_dataset.profile}\")\n",
    "print(f\"Dataset schema: {logged_dataset.schema}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3 and 4 components (create 2 runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"36d231c830034e888ad1e04dd67741d5\") as run:\n",
    "    pca = PCA(n_components=3)\n",
    "    pca3 = pca.fit_transform(scaled_features)\n",
    "\n",
    "    fig = sns.pairplot(pd.DataFrame(pca3), corner=True, kind=\"scatter\")\n",
    "\n",
    "    plt.savefig(\"./artifacts/pairplot_pca.png\")\n",
    "\n",
    "    mlflow.log_image(PIL.Image.open(\"./artifacts/pairplot_pca.png\"), \"artifacts/charts/pairplot_pca.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"36d231c830034e888ad1e04dd67741d5\") as run:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(x=pca3[:, 0], y=pca3[:, 1], hue=dates[:], legend=False, ax=ax)\n",
    "    plt.savefig(\"./artifacts/pca1_pca2\")\n",
    "    mlflow.log_figure(fig, \"artifacts/charts/pca1_pca2.png\")\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=pca3[:, 0], y=pca3[:, 2], hue=dates[:], legend=False, ax=ax)\n",
    "    plt.savefig(\"./artifacts/pca1_pca3\")\n",
    "    mlflow.log_figure(fig, \"artifacts/charts/pca1_pca3.png\")\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=pca3[:, 1], y=pca3[:, 2], hue=dates[:], legend=False, ax=ax)\n",
    "    plt.savefig(\"./artifacts/pca2_pca3\")\n",
    "    mlflow.log_figure(fig, \"artifacts/charts/pca2_pca3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"36d231c830034e888ad1e04dd67741d5\") as run:\n",
    "    pca = PCA(n_components=4)\n",
    "    pca4 = pca.fit_transform(scaled_features)\n",
    "    sns.pairplot(pd.DataFrame(pca4), corner=True, kind=\"scatter\")\n",
    "\n",
    "    fig = sns.pairplot(pd.DataFrame(pca4), corner=True, kind=\"scatter\")\n",
    "\n",
    "    plt.savefig(\"./artifacts/pairplot_pca4.png\")\n",
    "\n",
    "    mlflow.log_image(PIL.Image.open(\"./artifacts/pairplot_pca4.png\"), \"artifacts/charts/pairplot_pca4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READING REST OF THE SAMPLE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rest = pd.read_csv(SAMPLE_REST, skiprows=16, sep=';')\n",
    "df_rest.drop(columns=df_rest.columns[[0, 1]], inplace=True)\n",
    "\n",
    "column_names = pd.to_datetime(df_rest.iloc[0, ::2].tolist(), format='%d.%m.%Y %H:%M:%S') \n",
    "df_rest = df_rest.iloc[:, 1::2]\n",
    "df_rest.columns = column_names\n",
    "df_rest = df_rest.sort_index(axis=1)\n",
    "\n",
    "# Values to float\n",
    "df_rest = df_rest.apply(lambda x:\n",
    "              pd.to_numeric(\n",
    "                  x.str.replace(',','.'),\n",
    "                  errors='coerce')\n",
    "              )\n",
    "\n",
    "compress_dataset(df_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rest = df_rest.T.index\n",
    "feat_dataset_rest = pd.DataFrame()\n",
    "feat_dataset_rest[\"kurtosis\"] = df_rest.apply(lambda x: stats.kurtosis(x.astype(np.float64)))\n",
    "feat_dataset_rest[\"skewness\"] = df_rest.apply(lambda x: stats.skew(x.astype(np.float64)))\n",
    "feat_dataset_rest[\"peak2peak\"] = df_rest.apply(lambda x: np.ptp(x.astype(np.float64)))\n",
    "feat_dataset_rest[\"mean\"] = df_rest.apply(lambda x: x.astype(np.float64).mean())\n",
    "feat_dataset_rest[\"std\"] = df_rest.apply(lambda x: x.astype(np.float64).std())\n",
    "feat_dataset_rest[\"shapeFactor\"] = df_rest.apply(lambda x: np.sqrt(np.mean(np.square(x.astype(np.float64)))) / np.abs(x.astype(np.float64).mean()))\n",
    "feat_dataset_rest[\"energy\"] = df_rest.apply(lambda x: np.sum(x.astype(np.float64) ** 2))\n",
    "feat_dataset_rest.index = dates_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dataset_rest.to_csv(\"../data/processed/spectr/feature_dataset_rest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF columns to string to log dataset\n",
    "feat_dataset_rest.columns = feat_dataset_rest.columns.astype(str)\n",
    "\n",
    "RUN_NAME: str = \"PCA_SEACH_INDICATOR\"\n",
    "\n",
    "# GET ALL RUNS OF EXPERIMENT\n",
    "# ----------\n",
    "runs = mlflow.search_runs([CURRENT_EXPERIMENT.experiment_id])\n",
    "# CHECK IF EXPERIMENT EXISTS\n",
    "# ----------\n",
    "dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "    feat_dataset_rest, \n",
    "    name=\"FEATURE DATASET JANUARY-MARCH\",\n",
    "    source=\"\"\n",
    ")\n",
    "if runs[runs[\"tags.mlflow.runName\"] == RUN_NAME].shape[0] == 0:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"SEARCH HEALTH INDICATOR\")\n",
    "        # Log dataset csv\n",
    "        mlflow.log_artifact(\"../data/processed/spectr/feature_dataset.csv\", \"artifacts/data/feature_dataset.csv\")\n",
    "        # Log dataset as JSON\n",
    "        mlflow.log_table(feature_dataset, \"artifacts/data_json/feature_dataset.json\")\n",
    "else:\n",
    "    with mlflow.start_run(run_id=runs[runs[\"tags.mlflow.runName\"] == RUN_NAME]['run_id'][0]):\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"READ 6 MONTH RANGE\")\n",
    "        # Log dataset csv\n",
    "        mlflow.log_artifact(\"../data/processed/spectr/feature_dataset_rest.csv\", \"artifacts/data/feature_dataset_rest.csv\")\n",
    "        # Log dataset as JSON\n",
    "        mlflow.log_table(feat_dataset_rest, \"artifacts/data_json/feature_dataset_rest.json\")\n",
    "\n",
    "RUN_ID = run.info.run_id\n",
    "logged_run = mlflow.get_run(run.info.run_id)\n",
    "\n",
    "# Retrieve the Dataset object\n",
    "logged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n",
    "\n",
    "# View some of the recorded Dataset information\n",
    "print(f\"Dataset name: {logged_dataset.name}\")\n",
    "print(f\"Dataset digest: {logged_dataset.digest}\")\n",
    "print(f\"Dataset profile: {logged_dataset.profile}\")\n",
    "print(f\"Dataset schema: {logged_dataset.schema}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dataset_full = pd.concat([feature_dataset, feat_dataset_rest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dataset_full.to_csv(\"../data/processed/spectr/feature_dataset_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF columns to string to log dataset\n",
    "feat_dataset_full.columns = feat_dataset_full.columns.astype(str)\n",
    "\n",
    "RUN_NAME: str = \"PCA_SEACH_INDICATOR\"\n",
    "\n",
    "# GET ALL RUNS OF EXPERIMENT\n",
    "# ----------\n",
    "runs = mlflow.search_runs([CURRENT_EXPERIMENT.experiment_id])\n",
    "# CHECK IF EXPERIMENT EXISTS\n",
    "# ----------\n",
    "dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "    feat_dataset_full, \n",
    "    name=\"FEATURE DATASET JANUARY-MARCH\",\n",
    "    source=\"\"\n",
    ")\n",
    "if runs[runs[\"tags.mlflow.runName\"] == RUN_NAME].shape[0] == 0:\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"SEARCH HEALTH INDICATOR\")\n",
    "        # Log dataset csv\n",
    "        mlflow.log_artifact(\"../data/processed/spectr/feature_dataset.csv\", \"artifacts/data/feature_dataset.csv\")\n",
    "        # Log dataset as JSON\n",
    "        mlflow.log_table(feat_dataset_full, \"artifacts/data_json/feature_dataset.json\")\n",
    "else:\n",
    "    with mlflow.start_run(run_id=runs[runs[\"tags.mlflow.runName\"] == RUN_NAME]['run_id'][0]):\n",
    "        # Log dataset fields info\n",
    "        mlflow.log_input(dataset, context=\"CONCAT 2 PART OF DATASET\")\n",
    "        # Log dataset csv\n",
    "        mlflow.log_artifact(\"../data/processed/spectr/feature_dataset_full.csv\", \"artifacts/data/feature_dataset_full.csv\")\n",
    "        # Log dataset as JSON\n",
    "        mlflow.log_table(feat_dataset_full, \"artifacts/data_json/feature_dataset_full.json\")\n",
    "\n",
    "RUN_ID = run.info.run_id\n",
    "logged_run = mlflow.get_run(run.info.run_id)\n",
    "\n",
    "# Retrieve the Dataset object\n",
    "logged_dataset = logged_run.inputs.dataset_inputs[0].dataset\n",
    "\n",
    "# View some of the recorded Dataset information\n",
    "print(f\"Dataset name: {logged_dataset.name}\")\n",
    "print(f\"Dataset digest: {logged_dataset.digest}\")\n",
    "print(f\"Dataset profile: {logged_dataset.profile}\")\n",
    "print(f\"Dataset schema: {logged_dataset.schema}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"36d231c830034e888ad1e04dd67741d5\") as run:\n",
    "    fig, ax = plt.subplots(figsize=(16,16))\n",
    "    feat_dataset_full.plot(subplots=True, ax=ax)\n",
    "\n",
    "    mlflow.log_figure(fig, \"artifacts/charts/full_signals.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feat_dataset_full)\n",
    "\n",
    "pca = PCA().fit(scaled_features)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax.axhline(0.95, c=\"green\")\n",
    "ax.axvspan(3, 4, alpha=.5, color=\"red\")\n",
    "ax.set_xlabel('number of components')\n",
    "ax.set_ylabel('cumulative explained variance')\n",
    "ax.set_title(\"Number of components vs explained variance\")\n",
    "\n",
    "with mlflow.start_run(run_id=\"36d231c830034e888ad1e04dd67741d5\") as run:\n",
    "    mlflow.log_figure(fig, \"artifacts/charts/components_vs_variance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=\"36d231c830034e888ad1e04dd67741d5\") as run:\n",
    "    pca = PCA(n_components=4)\n",
    "    pca4 = pca.fit_transform(scaled_features)\n",
    "\n",
    "    fig = sns.pairplot(pd.DataFrame(pca4), corner=True, kind=\"scatter\")\n",
    "\n",
    "    plt.savefig(\"./artifacts/pairplot_pca_full.png\")\n",
    "\n",
    "    mlflow.log_image(PIL.Image.open(\"./artifacts/pairplot_pca_full.png\"), \"artifacts/charts/pairplot_pca_full.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
