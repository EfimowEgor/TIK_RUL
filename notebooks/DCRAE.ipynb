{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH: str = \"../data/raw/spectr/Выборка_Н22_5_1.csv\"\n",
    "SAMPLE_REST: str = \"../data/raw/spectr/СКЗУ_ВЫБОРКА_ЯНВАР-МАРТ.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SAMPLE_PATH, skiprows=16, sep=';')\n",
    "df.drop(columns=df.columns[[0, 1]], inplace=True)\n",
    "\n",
    "column_names = pd.to_datetime(df.iloc[0, ::2].tolist(), format='%d.%m.%Y %H:%M:%S') \n",
    "df = df.iloc[:, 1::2]\n",
    "df.columns = column_names\n",
    "df = df.sort_index(axis=1)\n",
    "\n",
    "df = df.apply(lambda x:\n",
    "              pd.to_numeric(\n",
    "                  x.str.replace(',','.'),\n",
    "                  errors='coerce')\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сжать выборки (Отцентровать и усреднить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: np.mean(x.to_numpy().reshape(-1, 257), axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rest = pd.read_csv(SAMPLE_REST, skiprows=16, sep=';')\n",
    "df_rest.drop(columns=df_rest.columns[[0, 1]], inplace=True)\n",
    "\n",
    "column_names = pd.to_datetime(df_rest.iloc[0, ::2].tolist(), format='%d.%m.%Y %H:%M:%S') \n",
    "df_rest = df_rest.iloc[:, 1::2]\n",
    "df_rest.columns = column_names\n",
    "df_rest = df_rest.sort_index(axis=1)\n",
    "\n",
    "df_rest = df_rest.apply(lambda x:\n",
    "                        pd.to_numeric(\n",
    "                            x.str.replace(',','.'),\n",
    "                            errors='coerce')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rest = df_rest.apply(lambda x: np.mean(x.to_numpy().reshape(-1, 257), axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём только первые 700 часов (т.е. до второй половины декабря 2022 года), потому что дальше что-то странное с данными выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединить "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_rest], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование спектра для построения индикатора здоровья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/spectr/RMSA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(max_val: int):\n",
    "    lin = np.linspace(0, max_val, max_val + 1)\n",
    "    return -(lin ** 2 / lin.max() ** 2) + 1\n",
    "\n",
    "def label_func2(max_val: int):\n",
    "    lin = np.linspace(0, max_val, max_val + 1)\n",
    "    return -(lin / lin.max()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аппроксимируем закон деградации (метки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_func(2224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение автоэнкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(encoder_output, encoder_target, decoder_output, decoder_target, w):\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    loss_encoder = criterion_mse(encoder_output, encoder_target)\n",
    "    loss_decoder = criterion_mse(decoder_output, decoder_target)\n",
    "    return loss_encoder + w * loss_decoder\n",
    "\n",
    "class CustomAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 20, kernel_size=3, stride=2, padding=1), \n",
    "            nn.MaxPool1d(2, stride=2),                         \n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(20, 40, kernel_size=3, stride=2, padding=1),   \n",
    "            nn.MaxPool1d(2, stride=2),                              \n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(40, 80, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.MaxPool1d(2, stride=2),                            \n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(80, 40, kernel_size=3, stride=2, padding=1),   \n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(40, 20, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(20, 10, kernel_size=3, stride=1, padding=1),   \n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(10, 1, kernel_size=3, stride=1, padding=1),   \n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool1d(kernel_size=3, dilation=1, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # L_in = 1\n",
    "            # L_out = (1 - 1) * 2 - 2 * 1 + 1 * (3 - 1) + 1 + 1\n",
    "            nn.ConvTranspose1d(1, 10, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 10, 2)\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(), \n",
    "            # L_in = 2\n",
    "            # L_out = (2 - 1) * 2 - 2 * 1 + 1 * (3 - 1) + 1 + 1\n",
    "            nn.ConvTranspose1d(10, 20, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 20, 4)\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(20, 40, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 40, 8)\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(40, 80, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 80, 16)\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(80, 40, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 40, 32)\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(40, 20, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 20, 64)\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(20, 10, kernel_size=3, stride=2, padding=1, output_padding=1), # (batch_size, 20, 128)\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(10, 1, kernel_size=9, stride=2, padding=1, output_padding=3, dilation=5), # (batch_size, 1, 296)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "def train(param, data, labels, num_epochs=100, w=1.0):\n",
    "    model = CustomAutoencoder()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=param['lr'])\n",
    "    if len(data.shape) == 2:\n",
    "        data = data.unsqueeze(1)  \n",
    "    \n",
    "    dataset = TensorDataset(data, labels)\n",
    "    train_loader = DataLoader(dataset, batch_size=param['batch_size'], shuffle=False)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, encoder_targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            encoder_outputs, decoder_outputs = model(inputs)\n",
    "            loss = criterion(encoder_outputs, encoder_targets, decoder_outputs, inputs, w)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Пример вызова\n",
    "param = {\n",
    "    'lr': 0.0005,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "data = torch.tensor(df.T.values, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "trained_model = train(param, data, labels, num_epochs=100, w=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HI = []\n",
    "trained_model.eval()\n",
    "with torch.no_grad():    \n",
    "    for i in range(2225):\n",
    "        HI.append(trained_model(torch.Tensor(df.T.iloc[i].to_numpy().reshape(1, 1, -1)))[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exponential_moving_average(data, window):\n",
    "#     weights = np.exp(np.linspace(-1., 0., window))\n",
    "#     weights /= weights.sum()\n",
    "#     ema = np.convolve(data, weights, mode='full')[:len(data)]\n",
    "#     ema[:window] = ema[window]\n",
    "#     return ema\n",
    "# sns.lineplot(exponential_moving_average(HI, 24))\n",
    "sns.lineplot(HI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получился мусор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
