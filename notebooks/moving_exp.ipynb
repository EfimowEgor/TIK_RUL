{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from DataLoader import (\n",
    "    config,\n",
    "    loader\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(signals: pd.DataFrame, floor: str='30min', method='max'):\n",
    "    \"\"\"\n",
    "    floor: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n",
    "    method: Определяет каким образом сжимается ряд. Принимает значения 'max' - максимум интервала, 'mean' - среднее значение интервала, 'mixed' - E(x) + max()\n",
    "    \"\"\"\n",
    "    match method:\n",
    "        case 'max':\n",
    "            return signals.groupby(signals.date.dt.floor(floor)).max().drop('date', axis=1)\n",
    "        case 'mean':\n",
    "            return signals.groupby(signals.date.dt.floor(floor)).mean().drop('date', axis=1)\n",
    "        case 'mixed': \n",
    "            pass\n",
    "            # convolve, how to optimize params? perceptron?\n",
    "        case _:\n",
    "            raise ValueError(f'Unknown method: {method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/'\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "# make a dict, key - file_name_last part\n",
    "dta = dict()\n",
    "datasets = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(data_path, file), skiprows=config.COUNT_SKIP, sep=';')\n",
    "    df = loader.fill_empty(loader.transform_header(df))\n",
    "    # Compress signal\n",
    "    compressed = compress(df, floor='5min', method='mean')\n",
    "    datasets.append(compressed)\n",
    "\n",
    "min_length = min(df.shape[0] for df in datasets)\n",
    "min_index = set(datasets[0].index)\n",
    "for df in datasets[1:]:\n",
    "    min_index.intersection_update(df.index)\n",
    "min_index = list(min_index)\n",
    "\n",
    "for i, df in enumerate(datasets):\n",
    "    datasets[i] = df.loc[min_index]\n",
    "\n",
    "# Проверка, что даты совпадают\n",
    "for i in range(len(files)):\n",
    "    for j in range(i + 1, len(files)):\n",
    "        assert np.setdiff1d(datasets[i].index, datasets[j].index).size == 0, f\"Intersection has shape {np.setdiff1d(datasets[i].index, datasets[j].index).shape}\"\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i].sort_index(inplace=True)\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "\n",
    "    splitted = loader.split(datasets[i].columns)\n",
    "    group = loader.group(splitted, datasets[i])\n",
    "\n",
    "    dta[file] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_moving_average(data, window):\n",
    "    weights = np.exp(np.linspace(-1., 0., window))\n",
    "    weights /= weights.sum()\n",
    "    ema = np.convolve(data, weights, mode='full')[:len(data)]\n",
    "    ema[:window] = ema[window]\n",
    "    return ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2time(rng: np.array):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 4, figsize=(14, 14))\n",
    "for i, file in enumerate(files):\n",
    "    j = 0 \n",
    "    for k in dta[file].keys():\n",
    "        for kk in dta[file][k].keys():\n",
    "            print(i, j, file, k, kk)\n",
    "            # print(dta[file][k][kk][len(dta[file][k][kk]) // 2:][0])\n",
    "            if len(dta[file][k][kk]) == 2:\n",
    "                sns.lineplot(exponential_moving_average(dta[file][k][kk][len(dta[file][k][kk]) // 2:][0], window=50), ax=ax[i][j])\n",
    "            else:\n",
    "                sns.lineplot(dta[file][k][kk][len(dta[file][k][kk]) // 2:], ax=ax[i][j])\n",
    "            # sns.kdeplot(dta[file][k][kk][len(dta[file][k][kk]) // 2:], ax=ax[i][j], color='green')\n",
    "            # sns.histplot(dta[file][k][kk][len(dta[file][k][kk]) // 2:], ax=ax[i][j])\n",
    "            # ax[i][j].set_xscale('log')\n",
    "            # ax[i][j].set_yscale('log')\n",
    "            ax[i][j].set_title(file + ' ' + k + ' ' + kk, fontsize=8)\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для сжатия точек до заданного промежутка времени - часы/дни. DONE (перенести в лоадер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В сгруппированном нет даты, а значит я должен сжимать по дате до группировки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "объединение данных по компонентам (перенести в лоадер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = defaultdict(list)\n",
    "for d in (dta.keys()):\n",
    "    for k_outer, v_outer in dta[d].items():\n",
    "        for k_inner, v_inner in v_outer.items():\n",
    "            # print(k_inner, v_inner)\n",
    "            dd[k_inner].append(v_inner)\n",
    "\n",
    "for key in dd.keys():\n",
    "    component_mat = np.array([])\n",
    "    for row in dd[key]:\n",
    "        data_row = np.array(row[len(row) // 2:])\n",
    "        if component_mat.size == 0:\n",
    "            component_mat = data_row\n",
    "        else:\n",
    "            component_mat = np.vstack([component_mat, data_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "определяем функцию деградации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_exp_custom(x, f, t, b, e, s):\n",
    "    return f + t * e ** (b * x + e - (s ** 2) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(np.arange(1, len(dd['ППН'][0][1]) + 1))\n",
    "for i in range(4):\n",
    "    if tmp.size == 0:\n",
    "        tmp = dd['ППН'][i][1]\n",
    "    else:\n",
    "        tmp = np.vstack([tmp, dd['ППН'][i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед PCA нормализуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tkeo_operator(data, k = 1):\n",
    "    \"\"\"\n",
    "    Teager-Kaiser Energy operator\n",
    "    \"\"\"\n",
    "    npnts = len(data[0])\n",
    "    nsignals = len(data)\n",
    "    filt_data = deepcopy(data)\n",
    "    for i in range(nsignals):\n",
    "        for n in range(k, npnts-k):\n",
    "            filt_data[i][n] = data[i][n]**2-data[i][n-1]*data[i][n+1]\n",
    "    return filt_data\n",
    "\n",
    "def normilize(signal: np.ndarray):\n",
    "    \"\"\"\n",
    "    MinMaxScaler + Teager-Kaiser Operator + MinMaxScaler\n",
    "    \"\"\"\n",
    "    # scalers = [MinMaxScaler, StandardScaler]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    signal = scaler.fit_transform(signal)\n",
    "    print(f'norm1 max: {signal.max()}, min: {signal.min()}')\n",
    "    signal = tkeo_operator(signal)\n",
    "    print(f'tkeo max: {signal.max()}, min: {signal.min()}')\n",
    "    signal = scaler.fit_transform(signal)\n",
    "    print(f'norm2 max: {signal.max()}, min: {signal.min()}')\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = normilize(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "compressed = pca.fit_transform(tmp.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(exponential_moving_average(abs(compressed[:, 1]), window=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curve fit (NON LINEAR LSTSQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xdata = np.arange(1, 4001)\n",
    "# popt, pcov = curve_fit(fit_exp_custom, xdata, exponential_moving_average(abs(compressed[:, 1][:4000]), window=100))\n",
    "\n",
    "# Чтобы так просто фитить нужно, чтобы в данных была экспонента, а у меня не так. Очевидно, что получится мусор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "Проверка кусочно-заданной, байесовская оптимизация параметров  \n",
    "Реализация индикатора здоровья через количество аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самописный байес для обновления параметров фукнции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчёт числа аномалий (граница - параметр функции). Экспоненциальный закон снижения остаточного ресурса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
